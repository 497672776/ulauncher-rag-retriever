"""
è½»é‡çº§æ£€ç´¢å™¨ - åªè´Ÿè´£æŸ¥è¯¢ï¼Œä¸è´Ÿè´£å»ºç«‹ç´¢å¼•
"""

import os
import pickle
from typing import List, Dict, Any

# ChromaDBç›¸å…³
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core import Settings
from llama_index.core.schema import TextNode
from llama_index.core.retrievers import VectorIndexRetriever

# BM25å’Œåˆ†è¯
from rank_bm25 import BM25Okapi
import jieba


class DocumentRetriever:
    """è½»é‡çº§æ–‡æ¡£æ£€ç´¢å™¨ - è´Ÿè´£æ–‡æ¡£æœç´¢å’Œç›¸å…³æ€§æ£€ç´¢"""
    
    def __init__(self, document_database_path: str):
        self.document_database_root_path = os.path.expanduser(document_database_path)
        
        # æ–‡æ¡£æ•°æ®å­˜å‚¨è·¯å¾„
        self.vector_database_path = os.path.join(self.document_database_root_path, "chroma_db")
        self.keyword_models_path = os.path.join(self.document_database_root_path, "keyword_model")
        
        # æ ¸å¿ƒæ£€ç´¢ç»„ä»¶
        self.chromadb_client = None
        self.vector_document_store = None
        self.vector_search_index = None
        self.bm25_keyword_search_model = None
        self.document_nodes_data = None
        self.text_embedding_model = None
        
        # æ£€ç´¢å™¨çŠ¶æ€ç®¡ç†
        self._is_data_loaded = False
        
    def is_document_search_available(self) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿæ˜¯å¦å¯ç”¨ä¸”æ•°æ®å®Œæ•´"""
        # æ£€æŸ¥æ–‡æ¡£æ£€ç´¢æ‰€éœ€çš„å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        essential_data_files = [
            self.vector_database_path,
            os.path.join(self.keyword_models_path, "bm25_model.pkl"),
            os.path.join(self.keyword_models_path, "nodes_data.pkl")
        ]
        
        for essential_file_path in essential_data_files:
            if not os.path.exists(essential_file_path):
                return False
                
        return True
        
        
    def _load_document_search_data_lazily(self):
        """å»¶è¿ŸåŠ è½½æ–‡æ¡£æ£€ç´¢æ•°æ®ï¼Œåªåœ¨éœ€è¦æ—¶åŠ è½½ä»¥æé«˜æ€§èƒ½"""
        if self._is_data_loaded:
            return
            
        print("ğŸ”„ åˆå§‹åŒ–æ–‡æ¡£æ£€ç´¢æ•°æ®...")
        
        try:
            # 1. åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
            self._initialize_text_embedding_model()
            
            # 2. åŠ è½½å‘é‡æ–‡æ¡£å­˜å‚¨
            self._load_vector_document_store()
            
            # 3. åŠ è½½BM25å…³é”®è¯æ£€ç´¢æ¨¡å‹
            self._load_bm25_keyword_search_model()
            
            self._is_data_loaded = True
            print("âœ… æ–‡æ¡£æ£€ç´¢æ•°æ®åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as initialization_error:
            print(f"âŒ æ–‡æ¡£æ£€ç´¢æ•°æ®åŠ è½½å¤±è´¥: {initialization_error}")
            raise
            
    def _initialize_text_embedding_model(self):
        """åˆå§‹åŒ–å’Œè®¾ç½®æ–‡æœ¬åµŒå…¥å‘é‡åŒ–æ¨¡å‹"""
        self.text_embedding_model = OllamaEmbedding(
            model_name="bge-m3:latest",
            base_url="http://localhost:11434",
            embed_batch_size=8,
            request_timeout=30,
        )
        Settings.embed_model = self.text_embedding_model
        
    def _load_vector_document_store(self):
        """åŠ è½½å‘é‡æ–‡æ¡£å­˜å‚¨å’Œæ£€ç´¢ç´¢å¼•"""
        self.chromadb_client = chromadb.PersistentClient(path=self.vector_database_path)
        document_collection_name = "documents"
        chroma_document_collection = self.chromadb_client.get_or_create_collection(name=document_collection_name)
        
        self.vector_document_store = ChromaVectorStore(chroma_collection=chroma_document_collection)
        vector_storage_context = StorageContext.from_defaults(vector_store=self.vector_document_store)
        
        self.vector_search_index = VectorStoreIndex.from_vector_store(
            self.vector_document_store,
            storage_context=vector_storage_context,
            embed_model=self.text_embedding_model
        )
        
    def _load_bm25_keyword_search_model(self):
        """åŠ è½½BM25å…³é”®è¯æ£€ç´¢æ¨¡å‹å’Œæ–‡æ¡£èŠ‚ç‚¹æ•°æ®"""
        bm25_model_file_path = os.path.join(self.keyword_models_path, "bm25_model.pkl")
        document_nodes_file_path = os.path.join(self.keyword_models_path, "nodes_data.pkl")
        
        with open(bm25_model_file_path, 'rb') as bm25_file:
            self.bm25_keyword_search_model = pickle.load(bm25_file)
            
        with open(document_nodes_file_path, 'rb') as nodes_file:
            serialized_nodes_data = pickle.load(nodes_file)
            
        # é‡æ–°æ„å»ºæ–‡æ¡£èŠ‚ç‚¹å¯¹è±¡
        self.document_nodes_data = []
        for node_data in serialized_nodes_data:
            document_node = TextNode(
                text=node_data['text'],
                metadata=node_data['metadata'],
                id_=node_data['node_id']
            )
            self.document_nodes_data.append(document_node)
            
    def search_documents(self, search_query: str, maximum_results: int = 9) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ··åˆæ–‡æ¡£æ£€ç´¢ - ç»“åˆå‘é‡ç›¸ä¼¼åº¦å’ŒBM25å…³é”®è¯æ£€ç´¢"""
        if not self.is_document_search_available():
            return []
            
        self._load_document_search_data_lazily()
        
        if not self._is_data_loaded:
            return []
            
        try:
            # åˆ›å»ºæ··åˆæ–‡æ¡£æ£€ç´¢å™¨ - ç»“åˆå‘é‡å’ŒBM25æ£€ç´¢
            hybrid_document_retriever = self._create_hybrid_document_retriever(similarity_top_k=maximum_results * 2)
            
            # æ‰§è¡Œæ··åˆæ–‡æ¡£æ£€ç´¢
            retrieved_document_nodes = hybrid_document_retriever.retrieve(search_query)
            
            # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
            formatted_search_results = []
            for document_node in retrieved_document_nodes:
                # è·å–æ–‡æ¡£æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                document_file_path = document_node.metadata.get('file_path', 'æœªçŸ¥è·¯å¾„')
                document_filename = os.path.basename(document_file_path) if document_file_path != 'æœªçŸ¥è·¯å¾„' else 'æœªçŸ¥æ–‡ä»¶'
                
                # è·å–æ–‡æ¡£ç›¸å…³æ€§åˆ†æ•°å’Œæ£€ç´¢æ¥æº
                relevance_score = 0.0
                if hasattr(document_node, 'metadata') and document_node.metadata and 'hybrid_score' in document_node.metadata:
                    relevance_score = document_node.metadata['hybrid_score']
                    search_method = document_node.metadata.get('retrieval_source', 'hybrid')
                else:
                    relevance_score = getattr(document_node, 'score', 0.0)
                    search_method = 'vector'
                
                # ç¡®ä¿ç›¸å…³æ€§åˆ†æ•°æ˜¯æœ‰æ•ˆæ•°å€¼
                try:
                    relevance_score = float(relevance_score) if relevance_score is not None else 0.0
                    if relevance_score < 0:
                        relevance_score = 0.0
                except (ValueError, TypeError):
                    relevance_score = 0.0
                
                formatted_search_results.append({
                    'rank': len(formatted_search_results) + 1,
                    'content': document_node.text.strip(),
                    'score': relevance_score,
                    'file_path': document_file_path,
                    'filename': document_filename,
                    'content_length': len(document_node.text),
                    'retrieval_source': search_method
                })
            
            # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åºå¹¶é™åˆ¶è¿”å›æ•°é‡
            formatted_search_results.sort(key=lambda x: x['score'], reverse=True)
            final_search_results = formatted_search_results[:maximum_results]
            
            # é‡æ–°è®¡ç®—æ’å
            for result_index, search_result in enumerate(final_search_results):
                search_result['rank'] = result_index + 1
                
            return final_search_results
            
        except Exception as search_error:
            print(f"âŒ æ–‡æ¡£æ£€ç´¢å¤±è´¥: {search_error}")
            return []
            
            
    def _create_hybrid_document_retriever(self, similarity_top_k: int = 10):
        """åˆ›å»ºæ··åˆæ–‡æ¡£æ£€ç´¢å™¨ - ç»“åˆå‘é‡ç›¸ä¼¼åº¦å’ŒBM25å…³é”®è¯æ£€ç´¢"""
        try:
            
            # 1. åˆ›å»ºå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢å™¨
            vector_similarity_retriever = VectorIndexRetriever(
                index=self.vector_search_index,
                similarity_top_k=similarity_top_k,
            )
            
            # 2. åˆ›å»ºBM25å…³é”®è¯æ£€ç´¢å™¨ï¼ˆéœ€è¦æ‰€æœ‰æ–‡æ¡£èŠ‚ç‚¹ï¼‰
            if not self.document_nodes_data:
                print("âš ï¸ æ²¡æœ‰æ–‡æ¡£èŠ‚ç‚¹å¯ç”¨äºBM25å…³é”®è¯æ£€ç´¢ï¼Œä»…ä½¿ç”¨å‘é‡æ£€ç´¢")
                return vector_similarity_retriever
                
            # å‡†å¤‡BM25å…³é”®è¯æ£€ç´¢è¯­æ–™åº“ï¼ˆä¸­æ–‡åˆ†è¯ï¼‰
            tokenized_document_corpus = []
            for document_node in self.document_nodes_data:
                # å¯¹æ–‡æ¡£èŠ‚ç‚¹è¿›è¡Œä¸­æ–‡åˆ†è¯
                document_tokens = list(jieba.cut(document_node.text, cut_all=False))
                tokenized_document_corpus.append(document_tokens)
            
            # åˆ›å»ºBM25å…³é”®è¯æ£€ç´¢æ¨¡å‹
            bm25_keyword_model = BM25Okapi(tokenized_document_corpus)
            
            # åˆ›å»ºè‡ªå®šä¹‰BM25å…³é”®è¯æ£€ç´¢å™¨
            class CustomBM25KeywordRetriever:
                def __init__(self, bm25_keyword_model, document_nodes, similarity_top_k=10):
                    self.bm25_keyword_search_model = bm25_keyword_model
                    self.document_nodes = document_nodes
                    self.maximum_similarity_results = similarity_top_k
                
                def retrieve(self, search_query_text):
                    # å¯¹æœç´¢æŸ¥è¯¢è¿›è¡Œä¸­æ–‡åˆ†è¯
                    tokenized_search_query = list(jieba.cut(search_query_text, cut_all=False))
                    
                    # æ‰§è¡ŒBM25å…³é”®è¯æ£€ç´¢
                    keyword_relevance_scores = self.bm25_keyword_search_model.get_scores(tokenized_search_query)
                    
                    # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£ç´¢å¼•
                    most_relevant_indices = keyword_relevance_scores.argsort()[-self.maximum_similarity_results:][::-1]
                    
                    # è¿”å›æ–‡æ¡£èŠ‚ç‚¹å’Œç›¸å…³æ€§åˆ†æ•°
                    keyword_search_results = []
                    for document_index in most_relevant_indices:
                        if document_index < len(self.document_nodes):
                            relevant_document_node = self.document_nodes[document_index]
                            keyword_search_results.append((relevant_document_node, float(keyword_relevance_scores[document_index])))
                    
                    return keyword_search_results
            
            bm25_keyword_retriever = CustomBM25KeywordRetriever(bm25_keyword_model, self.document_nodes_data, similarity_top_k)
            
            # 3. åˆ›å»ºç®€åŒ–çš„æ··åˆæ–‡æ¡£æ£€ç´¢å™¨
            class SimpleHybridDocumentRetriever:
                def __init__(self, vector_similarity_retriever, bm25_keyword_retriever, similarity_top_k=10):
                    self.vector_similarity_retriever = vector_similarity_retriever
                    self.bm25_keyword_retriever = bm25_keyword_retriever
                    self.maximum_results = similarity_top_k
                
                def retrieve(self, search_query_text):
                    # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
                    vector_similarity_results = self.vector_similarity_retriever.retrieve(search_query_text)
                    
                    # æ‰§è¡ŒBM25å…³é”®è¯æ£€ç´¢ï¼ˆè¿”å›(node, score)å…ƒç»„ï¼‰
                    bm25_keyword_results = self.bm25_keyword_retriever.retrieve(search_query_text)
                    
                    # åˆå¹¶ä¸¤ç§æ£€ç´¢ç»“æœï¼ˆä½¿ç”¨RRFèåˆç®—æ³•ï¼‰
                    merged_hybrid_results = {}
                    
                    # å¤„ç†å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ç»“æœ - ä½¿ç”¨æ–‡æ¡£å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    for result_rank, vector_node in enumerate(vector_similarity_results):
                        # ä½¿ç”¨æ–‡æ¡£å†…å®¹çš„å“ˆå¸Œå€¼ä½œä¸ºå”¯ä¸€æ ‡è¯†
                        document_key = hash(vector_node.text[:200])
                        reciprocal_rank_score = 1.0 / (result_rank + 1)  # å€’æ•°æ’åèåˆ
                        merged_hybrid_results[document_key] = {
                            'node': vector_node,
                            'score': reciprocal_rank_score,
                            'source': 'vector'
                        }
                    
                    # å¤„ç†BM25å…³é”®è¯æ£€ç´¢ç»“æœï¼ˆå¤„ç†(node, score)å…ƒç»„æ ¼å¼ï¼‰
                    for result_rank, bm25_item in enumerate(bm25_keyword_results):
                        if isinstance(bm25_item, tuple):
                            bm25_node, _ = bm25_item
                        else:
                            bm25_node = bm25_item
                            
                        # ä½¿ç”¨ç›¸åŒçš„æ–‡æ¡£èŠ‚ç‚¹æ ‡è¯†æ–¹æ³•
                        document_key = hash(bm25_node.text[:200])
                        reciprocal_rank_score = 1.0 / (result_rank + 1)
                        
                        if document_key in merged_hybrid_results:
                            # åˆå¹¶ä¸¤ç§æ£€ç´¢æ–¹æ³•çš„åˆ†æ•° - æ··åˆæ£€ç´¢çš„æ ¸å¿ƒ
                            merged_hybrid_results[document_key]['score'] += reciprocal_rank_score
                            merged_hybrid_results[document_key]['source'] = 'hybrid'
                        else:
                            merged_hybrid_results[document_key] = {
                                'node': bm25_node,
                                'score': reciprocal_rank_score,
                                'source': 'bm25'
                            }
                    
                    # æŒ‰èåˆç›¸å…³æ€§åˆ†æ•°æ’åº
                    sorted_hybrid_results = sorted(merged_hybrid_results.values(), 
                                          key=lambda x: x['score'], reverse=True)
                    
                    # è¿”å›å¸¦æœ‰æ··åˆåˆ†æ•°ä¿¡æ¯çš„æ–‡æ¡£èŠ‚ç‚¹åˆ—è¡¨
                    final_hybrid_results = []
                    for hybrid_result in sorted_hybrid_results[:self.maximum_results]:
                        original_document_node = hybrid_result['node']
                        # åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç›¸å…³æ€§åˆ†æ•°çš„æ–‡æ¡£èŠ‚ç‚¹å¯¹è±¡
                        class DocumentNodeWithHybridScore:
                            def __init__(self, document_node, hybrid_score, retrieval_source):
                                self.text = document_node.text
                                self.metadata = document_node.metadata.copy() if document_node.metadata else {}
                                self.metadata['hybrid_score'] = hybrid_score
                                self.metadata['retrieval_source'] = retrieval_source
                        
                        enriched_document_node = DocumentNodeWithHybridScore(
                            original_document_node, 
                            hybrid_result['score'], 
                            hybrid_result['source']
                        )
                        final_hybrid_results.append(enriched_document_node)
                    
                    return final_hybrid_results
            
            hybrid_document_retriever = SimpleHybridDocumentRetriever(
                vector_similarity_retriever, 
                bm25_keyword_retriever, 
                similarity_top_k
            )
            return hybrid_document_retriever
            
        except Exception as hybrid_retriever_error:
            print(f"âš ï¸ æ··åˆæ–‡æ¡£æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥ï¼Œé™çº§ä½¿ç”¨å•çº¯å‘é‡æ£€ç´¢: {str(hybrid_retriever_error)}")
            return VectorIndexRetriever(index=self.vector_search_index, similarity_top_k=similarity_top_k)