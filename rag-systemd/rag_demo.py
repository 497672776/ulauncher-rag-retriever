#!/usr/bin/env python3
"""
RAG Demo - ç®€åŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¼”ç¤º
åŠŸèƒ½ï¼šä»data_testç›®å½•åŠ è½½æ–‡æ¡£ï¼Œå®ç°æŸ¥è¯¢æ£€ç´¢ï¼Œè¿”å›ç›¸ä¼¼æ–‡æœ¬å—å’Œè·¯å¾„
"""

import os
import time
import pickle
import threading
from typing import List, Dict, Any
from pathlib import Path

# LlamaIndex imports
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.retrievers.fusion_retriever import QueryFusionRetriever
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core import Settings

# ChromaDB imports  
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext


class RAGDemo:
    """ç®€åŒ–çš„RAGæ¼”ç¤ºç³»ç»Ÿ"""
    
    def __init__(self, knowledge_base_path: str = "data_test", model_name: str = "bge-m3:latest"):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            knowledge_base_path: çŸ¥è¯†åº“æ–‡æ¡£ç›®å½•è·¯å¾„
            model_name: åµŒå…¥æ¨¡å‹åç§°
        """
        self.knowledge_base_path = knowledge_base_path
        self.model_name = model_name
        self.chroma_db_path = "./demo_chroma_db/chroma_db"
        self.bm25_cache_path = "./demo_chroma_db/models/bm25_model.pkl"
        self.nodes_cache_path = "./demo_chroma_db/models/nodes_data.pkl"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.knowledge_base_path, exist_ok=True)
        os.makedirs(self.chroma_db_path, exist_ok=True)
        os.makedirs(os.path.dirname(self.bm25_cache_path), exist_ok=True)
        
        print(f"ğŸš€ åˆå§‹åŒ–RAG Demo")
        print(f"ğŸ“ çŸ¥è¯†åº“è·¯å¾„: {self.knowledge_base_path}")
        print(f"ğŸ¤– åµŒå…¥æ¨¡å‹: {self.model_name}")
        print(f"ğŸ—ƒï¸ å‘é‡æ•°æ®åº“: {self.chroma_db_path}")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._setup_embedding_model()
        self._setup_vector_store()
        self.index = None
        self.documents_metadata = []
        self.all_nodes = []  # å­˜å‚¨æ‰€æœ‰èŠ‚ç‚¹ç”¨äºBM25æ£€ç´¢
        
        # æ–‡ä»¶ç›‘æ§ç›¸å…³
        self._monitoring = False
        self._monitor_thread = None
        self._known_files = set()  # å·²çŸ¥æ–‡ä»¶é›†åˆ
        self._supported_extensions = {'.txt', '.md', '.pdf', '.docx', '.doc'}
        
    def _setup_embedding_model(self):
        """è®¾ç½®åµŒå…¥æ¨¡å‹"""
        print("ğŸ”§ é…ç½®åµŒå…¥æ¨¡å‹...")
        self.embed_model = OllamaEmbedding(
            model_name=self.model_name,
            base_url="http://localhost:11434",
            embed_batch_size=8,
            request_timeout=30,
        )
        
        # è®¾ç½®å…¨å±€é…ç½®
        Settings.embed_model = self.embed_model
        Settings.chunk_size = 300  # ç¨å¤§çš„å—ä»¥è·å¾—æ›´å¤šä¸Šä¸‹æ–‡
        Settings.chunk_overlap = 50
        
    def _setup_vector_store(self):
        """è®¾ç½®ChromaDBå‘é‡å­˜å‚¨"""
        print("ğŸ—ƒï¸ é…ç½®ChromaDB...")
        print(f"ğŸ” æ•°æ®åº“è·¯å¾„: {os.path.abspath(self.chroma_db_path)}")
        self.chroma_client = chromadb.PersistentClient(path=self.chroma_db_path)
        
        # è·å–æˆ–åˆ›å»ºé›†åˆ
        collection_name = "rag_demo"
        self.chroma_collection = self.chroma_client.get_or_create_collection(name=collection_name)
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
        self.storage_context = StorageContext.from_defaults(vector_store=self.vector_store)
    
    def _check_document_exists(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²åœ¨å‘é‡åº“ä¸­å­˜åœ¨
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            True if document exists, False otherwise
        """
        try:
            # æŸ¥è¯¢ChromaDBä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥æ–‡æ¡£è·¯å¾„
            results = self.chroma_collection.get(
                where={"file_path": file_path},
                limit=1
            )
            
            exists = len(results['ids']) > 0
            if exists:
                filename = os.path.basename(file_path)
                print(f"ğŸ“‹ æ–‡æ¡£å·²å­˜åœ¨äºå‘é‡åº“: {filename}")
            
            return exists
            
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥æ–‡æ¡£å­˜åœ¨æ€§å¤±è´¥ {file_path}: {e}")
            return False
    
    def _get_existing_documents(self) -> List[str]:
        """
        è·å–å·²å­˜åœ¨äºå‘é‡åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£è·¯å¾„
        
        Returns:
            å·²å­˜åœ¨æ–‡æ¡£çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            # æŸ¥è¯¢æ‰€æœ‰æ–‡æ¡£çš„å…ƒæ•°æ®
            results = self.chroma_collection.get(
                include=['metadatas']
            )
            
            existing_files = []
            for metadata in results['metadatas']:
                if metadata and 'file_path' in metadata:
                    existing_files.append(metadata['file_path'])
            
            return list(set(existing_files))  # å»é‡
            
        except Exception as e:
            print(f"âš ï¸ è·å–å·²å­˜åœ¨æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def _save_bm25_model(self, bm25_model, nodes: List) -> None:
        """
        ä¿å­˜BM25æ¨¡å‹å’ŒèŠ‚ç‚¹æ•°æ®åˆ°ç£ç›˜
        
        Args:
            bm25_model: BM25æ¨¡å‹å¯¹è±¡
            nodes: èŠ‚ç‚¹åˆ—è¡¨
        """
        try:
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            cache_dir = os.path.dirname(self.bm25_cache_path)
            os.makedirs(cache_dir, exist_ok=True)
            
            # ä¿å­˜BM25æ¨¡å‹
            with open(self.bm25_cache_path, 'wb') as f:
                pickle.dump(bm25_model, f)
            
            # ä¿å­˜èŠ‚ç‚¹æ•°æ®
            nodes_data = []
            for node in nodes:
                nodes_data.append({
                    'text': node.text,
                    'metadata': node.metadata,
                    'node_id': node.node_id
                })
            
            with open(self.nodes_cache_path, 'wb') as f:
                pickle.dump(nodes_data, f)
            
            print(f"ğŸ’¾ BM25æ¨¡å‹å’ŒèŠ‚ç‚¹æ•°æ®å·²ä¿å­˜åˆ°ç£ç›˜")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜BM25æ¨¡å‹å¤±è´¥: {e}")
    
    def _load_bm25_model(self):
        """
        ä»ç£ç›˜åŠ è½½BM25æ¨¡å‹å’ŒèŠ‚ç‚¹æ•°æ®
        
        Returns:
            tuple: (bm25_model, nodes) æˆ– (None, None) å¦‚æœåŠ è½½å¤±è´¥
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not (os.path.exists(self.bm25_cache_path) and os.path.exists(self.nodes_cache_path)):
                return None, None
            
            # åŠ è½½BM25æ¨¡å‹
            with open(self.bm25_cache_path, 'rb') as f:
                bm25_model = pickle.load(f)
            
            # åŠ è½½èŠ‚ç‚¹æ•°æ®
            with open(self.nodes_cache_path, 'rb') as f:
                nodes_data = pickle.load(f)
            
            # é‡å»ºèŠ‚ç‚¹å¯¹è±¡
            from llama_index.core.schema import TextNode
            nodes = []
            for node_data in nodes_data:
                node = TextNode(
                    text=node_data['text'],
                    metadata=node_data['metadata'],
                    id_=node_data['node_id']
                )
                nodes.append(node)
            
            print(f"ğŸ“‚ ä»ç£ç›˜åŠ è½½BM25æ¨¡å‹å’Œ {len(nodes)} ä¸ªèŠ‚ç‚¹")
            return bm25_model, nodes
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½BM25æ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def _update_bm25_model(self, existing_bm25, existing_nodes: List, new_nodes: List):
        """
        æ›´æ–°BM25æ¨¡å‹ï¼Œæ·»åŠ æ–°æ–‡æ¡£
        
        Args:
            existing_bm25: ç°æœ‰BM25æ¨¡å‹
            existing_nodes: ç°æœ‰èŠ‚ç‚¹åˆ—è¡¨
            new_nodes: æ–°èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            tuple: (updated_bm25_model, all_nodes)
        """
        try:
            from rank_bm25 import BM25Okapi
            import jieba
            
            # åˆå¹¶æ‰€æœ‰èŠ‚ç‚¹
            all_nodes = existing_nodes + new_nodes
            
            # é‡æ–°æ„å»ºBM25æ¨¡å‹ï¼ˆåŒ…å«æ‰€æœ‰æ–‡æ¡£ï¼‰
            print("ğŸ”„ æ›´æ–°BM25æ¨¡å‹...")
            tokenized_corpus = []
            for node in all_nodes:
                tokens = list(jieba.cut(node.text))
                tokenized_corpus.append(tokens)
            
            bm25_model = BM25Okapi(tokenized_corpus)
            print(f"âœ… BM25æ¨¡å‹å·²æ›´æ–°ï¼ŒåŒ…å« {len(all_nodes)} ä¸ªæ–‡æ¡£")
            
            return bm25_model, all_nodes
            
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°BM25æ¨¡å‹å¤±è´¥: {e}")
            return None, existing_nodes
    
    def create_hybrid_retriever(self, similarity_top_k: int = 10, num_queries: int = 4):
        """åˆ›å»ºæ··åˆæ£€ç´¢å™¨ï¼šå‘é‡æ£€ç´¢ + BM25å…³é”®è¯æ£€ç´¢"""
        try:
            # 1. åˆ›å»ºå‘é‡æ£€ç´¢å™¨
            vector_retriever = VectorIndexRetriever(
                index=self.index,
                similarity_top_k=similarity_top_k,
            )
            
            # 2. åˆ›å»ºBM25æ£€ç´¢å™¨ï¼ˆéœ€è¦æ‰€æœ‰èŠ‚ç‚¹ï¼‰
            if not self.all_nodes:
                print("âš ï¸ æ²¡æœ‰èŠ‚ç‚¹å¯ç”¨äºBM25æ£€ç´¢ï¼Œä»…ä½¿ç”¨å‘é‡æ£€ç´¢")
                return vector_retriever
            
            from rank_bm25 import BM25Okapi
            import jieba
            
            # å‡†å¤‡BM25è¯­æ–™åº“ï¼ˆä¸­æ–‡åˆ†è¯ï¼‰
            tokenized_corpus = []
            for node in self.all_nodes:
                # ä¸­æ–‡åˆ†è¯
                tokens = list(jieba.cut(node.text, cut_all=False))
                tokenized_corpus.append(tokens)
            
            # åˆ›å»ºBM25æ¨¡å‹
            bm25 = BM25Okapi(tokenized_corpus)
            
            # åˆ›å»ºè‡ªå®šä¹‰BM25æ£€ç´¢å™¨
            class CustomBM25Retriever:
                def __init__(self, bm25_model, nodes, similarity_top_k=10):
                    self.bm25 = bm25_model
                    self.nodes = nodes
                    self.similarity_top_k = similarity_top_k
                
                def retrieve(self, query_str):
                    # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
                    tokenized_query = list(jieba.cut(query_str, cut_all=False))
                    
                    # BM25æ£€ç´¢
                    scores = self.bm25.get_scores(tokenized_query)
                    
                    # è·å–top_kç»“æœ
                    top_indices = scores.argsort()[-self.similarity_top_k:][::-1]
                    
                    # è¿”å›èŠ‚ç‚¹å’Œåˆ†æ•°ä¿¡æ¯ï¼Œä¸ç›´æ¥ä¿®æ”¹èŠ‚ç‚¹
                    results = []
                    for idx in top_indices:
                        if idx < len(self.nodes):
                            node = self.nodes[idx]
                            # åˆ›å»ºèŠ‚ç‚¹å‰¯æœ¬æˆ–ä½¿ç”¨tupleå­˜å‚¨åˆ†æ•°ä¿¡æ¯
                            results.append((node, float(scores[idx])))
                    
                    return results
            
            bm25_retriever = CustomBM25Retriever(bm25, self.all_nodes, similarity_top_k)
            
            # 3. åˆ›å»ºç®€åŒ–çš„æ··åˆæ£€ç´¢å™¨
            class SimpleHybridRetriever:
                def __init__(self, vector_retriever, bm25_retriever, similarity_top_k=10):
                    self.vector_retriever = vector_retriever
                    self.bm25_retriever = bm25_retriever
                    self.similarity_top_k = similarity_top_k
                
                def retrieve(self, query_str):
                    # æ‰§è¡Œå‘é‡æ£€ç´¢
                    vector_results = self.vector_retriever.retrieve(query_str)
                    
                    # æ‰§è¡ŒBM25æ£€ç´¢ï¼ˆè¿”å›(node, score)å…ƒç»„ï¼‰
                    bm25_results = self.bm25_retriever.retrieve(query_str)
                    
                    # åˆå¹¶ç»“æœï¼ˆç®€å•RRFèåˆï¼‰
                    all_results = {}
                    
                    # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ - ä½¿ç”¨æ–‡æœ¬å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†
                    for i, node in enumerate(vector_results):
                        # ä½¿ç”¨æ–‡æœ¬å†…å®¹çš„å“ˆå¸Œå€¼ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œæ›´å¯é 
                        node_key = hash(node.text[:200])  # ä½¿ç”¨å‰200å­—ç¬¦çš„å“ˆå¸Œé¿å…å®Œå…¨ç›¸åŒçš„é•¿æ–‡æœ¬
                        rrf_score = 1.0 / (i + 1)  # å€’æ•°æ’åèåˆ
                        all_results[node_key] = {
                            'node': node,
                            'score': rrf_score,
                            'source': 'vector'
                        }
                    
                    # å¤„ç†BM25æ£€ç´¢ç»“æœï¼ˆå¤„ç†(node, score)å…ƒç»„æ ¼å¼ï¼‰
                    for i, item in enumerate(bm25_results):
                        if isinstance(item, tuple):
                            node, bm25_score = item
                        else:
                            node = item
                            bm25_score = 0.0
                            
                        # ä½¿ç”¨ç›¸åŒçš„èŠ‚ç‚¹æ ‡è¯†æ–¹æ³•
                        node_key = hash(node.text[:200])
                        rrf_score = 1.0 / (i + 1)
                        
                        if node_key in all_results:
                            # åˆå¹¶åˆ†æ•° - è¿™é‡Œæ‰æ˜¯çœŸæ­£çš„æ··åˆæ£€ç´¢
                            all_results[node_key]['score'] += rrf_score
                            all_results[node_key]['source'] = 'hybrid'
                            print(f"ğŸ”¥ å‘ç°æ··åˆæ£€ç´¢èŠ‚ç‚¹: {node.text[:50]}...")  # è°ƒè¯•ä¿¡æ¯
                        else:
                            all_results[node_key] = {
                                'node': node,
                                'score': rrf_score,
                                'source': 'bm25'
                            }
                    
                    # æŒ‰èåˆåˆ†æ•°æ’åº
                    sorted_results = sorted(all_results.values(), 
                                          key=lambda x: x['score'], reverse=True)
                    
                    print(f"ğŸ” æ£€ç´¢ç»Ÿè®¡: Vector={len(vector_results)}, BM25={len(bm25_results)}, æ€»è®¡={len(all_results)}")
                    
                    # ç»Ÿè®¡å„ç±»å‹ç»“æœæ•°é‡
                    source_count = {'vector': 0, 'bm25': 0, 'hybrid': 0}
                    for result in sorted_results:
                        source_count[result['source']] += 1
                    print(f"ğŸ“Š ç»“æœåˆ†å¸ƒ: {source_count}")
                    
                    # è¿”å›å¸¦æœ‰ä¸´æ—¶åˆ†æ•°ä¿¡æ¯çš„èŠ‚ç‚¹åˆ—è¡¨
                    final_results = []
                    for result in sorted_results[:self.similarity_top_k]:
                        node = result['node']
                        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ç»“æœå¯¹è±¡ï¼ŒåŒ…å«èŠ‚ç‚¹å’Œåˆ†æ•°ä¿¡æ¯
                        class NodeWithScore:
                            def __init__(self, node, score, source):
                                self.text = node.text
                                self.metadata = node.metadata.copy() if node.metadata else {}
                                self.metadata['hybrid_score'] = score
                                self.metadata['retrieval_source'] = source
                        
                        result_node = NodeWithScore(node, result['score'], result['source'])
                        final_results.append(result_node)
                    
                    return final_results
            
            hybrid_retriever = SimpleHybridRetriever(vector_retriever, bm25_retriever, similarity_top_k)
            
            print(f"ğŸ” åˆ›å»ºæ··åˆæ£€ç´¢å™¨: å‘é‡æ£€ç´¢ + BM25å…³é”®è¯æ£€ç´¢")
            return hybrid_retriever
            
        except Exception as e:
            print(f"âš ï¸ æ··åˆæ£€ç´¢å™¨åˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨å‘é‡æ£€ç´¢: {str(e)}")
            return VectorIndexRetriever(index=self.index, similarity_top_k=similarity_top_k)
        
    def load_documents(self) -> int:
        """
        ä»knowledge_base_pathåŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼ˆå¸¦å»é‡æ£€æŸ¥ï¼‰
        
        Returns:
            åŠ è½½çš„æ–‡æ¡£æ•°é‡
        """
        print(f"ğŸ“„ åŠ è½½æ–‡æ¡£ä»: {self.knowledge_base_path}")
        start_time = time.time()
        
        # ä½¿ç”¨SimpleDirectoryReaderåŠ è½½æ–‡æ¡£ - é€ä¸ªæ–‡ä»¶åŠ è½½ä»¥é¿å…ç›®å½•æ¨¡å¼é—®é¢˜
        try:
            documents = []
            supported_exts = [".txt", ".md", ".pdf", ".docx", ".doc"]
            
            # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            for root, _, files in os.walk(self.knowledge_base_path):
                for file in files:
                    if any(file.lower().endswith(ext) for ext in supported_exts):
                        file_path = os.path.join(root, file)
                        try:
                            # é€ä¸ªæ–‡ä»¶åŠ è½½
                            reader = SimpleDirectoryReader(input_files=[file_path])
                            file_docs = reader.load_data()
                            documents.extend(file_docs)
                            print(f"âœ… åŠ è½½æ–‡ä»¶: {file} ({len(file_docs)} ä¸ªæ–‡æ¡£)")
                        except Exception as e:
                            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file}: {e}")
            
            if not documents:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
                return 0
                
            print(f"âœ… æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
            
            # æ£€æŸ¥å“ªäº›æ–‡æ¡£æ˜¯æ–°çš„
            print("ğŸ” æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨äºå‘é‡åº“...")
            new_documents = []
            skipped_count = 0
            
            for doc in documents:
                file_path = doc.metadata.get('file_path', 'æœªçŸ¥è·¯å¾„')
                filename = os.path.basename(file_path)
                
                # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
                if self._check_document_exists(file_path):
                    skipped_count += 1
                    continue
                else:
                    new_documents.append(doc)
                    print(f"   ğŸ“„ æ–°æ–‡æ¡£: {filename} ({len(doc.text)} å­—ç¬¦)")
            
            print(f"ğŸ“Š ç»Ÿè®¡: æ–°æ–‡æ¡£ {len(new_documents)} ä¸ªï¼Œè·³è¿‡å·²å­˜åœ¨ {skipped_count} ä¸ª")
            
            if not new_documents:
                print("âœ… æ‰€æœ‰æ–‡æ¡£éƒ½å·²å­˜åœ¨äºå‘é‡åº“ä¸­ï¼Œæ— éœ€é‡æ–°å¤„ç†")
                # ä»éœ€è¦åŠ è½½ç°æœ‰ç´¢å¼•ç”¨äºæŸ¥è¯¢
                try:
                    self.index = VectorStoreIndex.from_vector_store(
                        self.vector_store,
                        storage_context=self.storage_context,
                        embed_model=self.embed_model
                    )
                    print("ğŸ” åŠ è½½ç°æœ‰å‘é‡ç´¢å¼•")
                    
                    # å°è¯•ä»ç£ç›˜åŠ è½½BM25æ¨¡å‹å’ŒèŠ‚ç‚¹
                    print("ğŸ“ åŠ è½½BM25æ¨¡å‹å’ŒèŠ‚ç‚¹...")
                    bm25_model, cached_nodes = self._load_bm25_model()
                    
                    if bm25_model is not None and cached_nodes is not None:
                        # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®
                        self.all_nodes.extend(cached_nodes)
                        print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½ {len(cached_nodes)} ä¸ªèŠ‚ç‚¹ç”¨äºBM25æ£€ç´¢")
                    else:
                        # ç¼“å­˜ä¸å­˜åœ¨ï¼Œé‡æ–°ç”Ÿæˆ
                        print("ğŸ“ é‡æ–°ç”ŸæˆèŠ‚ç‚¹ç”¨äºBM25æ£€ç´¢...")
                        parser = SimpleNodeParser.from_defaults(
                            chunk_size=300,
                            chunk_overlap=50
                        )
                        all_nodes = parser.get_nodes_from_documents(documents)
                        self.all_nodes.extend(all_nodes)
                        print(f"ğŸ“ ç”Ÿæˆ {len(all_nodes)} ä¸ªèŠ‚ç‚¹ç”¨äºBM25æ£€ç´¢")
                        
                        # åˆ›å»ºå¹¶ä¿å­˜BM25æ¨¡å‹
                        from rank_bm25 import BM25Okapi
                        import jieba
                        
                        tokenized_corpus = []
                        for node in all_nodes:
                            tokens = list(jieba.cut(node.text))
                            tokenized_corpus.append(tokens)
                        
                        bm25_model = BM25Okapi(tokenized_corpus)
                        self._save_bm25_model(bm25_model, all_nodes)
                    
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•åŠ è½½ç°æœ‰ç´¢å¼•: {e}")
                
                return len(documents)
            
            # ä¿å­˜æ‰€æœ‰æ–‡æ¡£å…ƒæ•°æ®ï¼ˆåŒ…æ‹¬å·²å­˜åœ¨çš„ï¼‰
            self.documents_metadata = []
            for doc in documents:
                file_path = doc.metadata.get('file_path', 'æœªçŸ¥è·¯å¾„')
                filename = os.path.basename(file_path)
                self.documents_metadata.append({
                    'filename': filename,
                    'file_path': file_path,
                    'content_length': len(doc.text)
                })
            
            # åªå¤„ç†æ–°æ–‡æ¡£
            print("âœ‚ï¸ åˆ‡åˆ†æ–°æ–‡æ¡£...")
            parser = SimpleNodeParser.from_defaults(
                chunk_size=300,
                chunk_overlap=50
            )
            new_nodes = parser.get_nodes_from_documents(new_documents)
            print(f"ğŸ“Š æ–°ç”Ÿæˆ {len(new_nodes)} ä¸ªæ–‡æœ¬å—")
            
            # æ–°èŠ‚ç‚¹å·²åŒ…å«å¿…è¦çš„å…ƒæ•°æ®ä¿¡æ¯
            
            # å¤„ç†BM25æ¨¡å‹çš„å¢é‡æ›´æ–°
            print("ğŸ”„ å¤„ç†BM25æ¨¡å‹...")
            existing_bm25, existing_nodes = self._load_bm25_model()
            
            if existing_bm25 is not None and existing_nodes is not None:
                # æ›´æ–°ç°æœ‰BM25æ¨¡å‹
                updated_bm25, all_nodes = self._update_bm25_model(existing_bm25, existing_nodes, new_nodes)
                if updated_bm25 is not None:
                    self.all_nodes = all_nodes
                    self._save_bm25_model(updated_bm25, all_nodes)
                    print(f"ğŸ“ BM25æ¨¡å‹å·²æ›´æ–°ï¼Œæ€»å…± {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
                else:
                    # æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨ç°æœ‰èŠ‚ç‚¹åŠ æ–°èŠ‚ç‚¹
                    self.all_nodes = existing_nodes + new_nodes
                    print(f"âš ï¸ BM25æ¨¡å‹æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨ç°æœ‰æ¨¡å‹ï¼Œæ€»å…± {len(self.all_nodes)} ä¸ªèŠ‚ç‚¹")
            else:
                # åˆ›å»ºæ–°çš„BM25æ¨¡å‹
                print("ğŸ†• åˆ›å»ºæ–°BM25æ¨¡å‹...")
                all_nodes = new_nodes.copy()
                
                # å¦‚æœæœ‰å…¶ä»–å·²å­˜åœ¨çš„æ–‡æ¡£ï¼Œä¹Ÿè¦åŒ…å«è¿›æ¥
                if skipped_count > 0:
                    print("ğŸ“ é‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£çš„èŠ‚ç‚¹...")
                    parser_all = SimpleNodeParser.from_defaults(
                        chunk_size=300,
                        chunk_overlap=50
                    )
                    all_document_nodes = parser_all.get_nodes_from_documents(documents)
                    all_nodes = all_document_nodes
                
                from rank_bm25 import BM25Okapi
                import jieba
                
                tokenized_corpus = []
                for node in all_nodes:
                    tokens = list(jieba.cut(node.text))
                    tokenized_corpus.append(tokens)
                
                bm25_model = BM25Okapi(tokenized_corpus)
                self._save_bm25_model(bm25_model, all_nodes)
                self.all_nodes = all_nodes
                print(f"ğŸ“ æ–°BM25æ¨¡å‹å·²åˆ›å»ºï¼ŒåŒ…å« {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
            
            # åˆ›å»ºæˆ–æ›´æ–°å‘é‡ç´¢å¼•
            print("ğŸ”„ æ›´æ–°å‘é‡ç´¢å¼•...")
            embed_start = time.time()
            
            try:
                # å°è¯•ä»ç°æœ‰å­˜å‚¨åŠ è½½
                self.index = VectorStoreIndex.from_vector_store(
                    self.vector_store,
                    storage_context=self.storage_context,
                    embed_model=self.embed_model
                )
                print("ğŸ” ä»ç°æœ‰ChromaDBåŠ è½½ç´¢å¼•")
                
                # åªæ·»åŠ æ–°èŠ‚ç‚¹
                if new_nodes:
                    self.index.insert_nodes(new_nodes)
                    print(f"â• æ·»åŠ äº† {len(new_nodes)} ä¸ªæ–°æ–‡æœ¬å—")
                
            except:
                # åˆ›å»ºæ–°ç´¢å¼•
                self.index = VectorStoreIndex(
                    new_nodes,
                    storage_context=self.storage_context, 
                    embed_model=self.embed_model
                )
                print("ğŸ†• åˆ›å»ºæ–°çš„å‘é‡ç´¢å¼•")
            
            embed_time = time.time() - embed_start
            total_time = time.time() - start_time
            
            print(f"âœ… å‘é‡åŒ–å®Œæˆ: {embed_time:.2f}ç§’")
            print(f"ğŸ‰ æ–‡æ¡£åŠ è½½æ€»è€—æ—¶: {total_time:.2f}ç§’")
            
            return len(documents)
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            return 0
    
    def query(self, question: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            åŒ…å«ç›¸ä¼¼æ–‡æœ¬å—å’Œè·¯å¾„çš„ç»“æœåˆ—è¡¨
        """
        if not self.index:
            print("âŒ è¯·å…ˆåŠ è½½æ–‡æ¡£")
            return []
        
        print(f"ğŸ” æŸ¥è¯¢: {question}")
        start_time = time.time()
        
        try:
            # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
            retriever = self.create_hybrid_retriever(similarity_top_k=top_k * 2)  # è·å–æ›´å¤šå€™é€‰
            
            # æ‰§è¡Œæ··åˆæ£€ç´¢
            nodes = retriever.retrieve(question)
            
            # å¤„ç†ç»“æœï¼Œæ­£ç¡®è·å–ç›¸ä¼¼åº¦å¾—åˆ†
            results = []
            for node in nodes:
                # è·å–æ–‡ä»¶è·¯å¾„
                file_path = node.metadata.get('file_path', 'æœªçŸ¥è·¯å¾„')
                filename = os.path.basename(file_path) if file_path != 'æœªçŸ¥è·¯å¾„' else 'æœªçŸ¥æ–‡ä»¶'
                
                # è·å–åˆ†æ•°ï¼šä¼˜å…ˆä»metadataè·å–æ··åˆåˆ†æ•°ï¼Œå¦åˆ™ä½¿ç”¨èŠ‚ç‚¹åŸæœ‰åˆ†æ•°
                score = 0.0
                if hasattr(node, 'metadata') and node.metadata and 'hybrid_score' in node.metadata:
                    # ä½¿ç”¨æ··åˆæ£€ç´¢åˆ†æ•°
                    score = node.metadata['hybrid_score']
                    retrieval_source = node.metadata.get('retrieval_source', 'hybrid')
                else:
                    # ä½¿ç”¨åŸå§‹åˆ†æ•°
                    score = getattr(node, 'score', 0.0)
                    retrieval_source = 'vector'
                
                # ç¡®ä¿scoreæ˜¯æµ®ç‚¹æ•°å¹¶ä¸”åœ¨åˆç†èŒƒå›´å†…
                try:
                    score = float(score) if score is not None else 0.0
                    if score < 0:
                        score = 0.0
                except (ValueError, TypeError):
                    score = 0.0
                
                results.append({
                    'rank': len(results) + 1,
                    'content': node.text.strip(),
                    'score': score,
                    'file_path': file_path,
                    'filename': filename,
                    'content_length': len(node.text),
                    'retrieval_source': retrieval_source  # æ·»åŠ æ£€ç´¢æºä¿¡æ¯
                })
            
            # æŒ‰ç›¸ä¼¼åº¦å¾—åˆ†æ’åºï¼ˆé™åºï¼‰å¹¶é™åˆ¶è¿”å›æ•°é‡
            results.sort(key=lambda x: x['score'], reverse=True)
            results = results[:top_k]  # é™åˆ¶è¿”å›æ•°é‡
            
            # é‡æ–°åˆ†é…æ’å
            for i, result in enumerate(results):
                result['rank'] = i + 1
            
            search_time = time.time() - start_time
            
            print(f"âœ… æ··åˆæ£€ç´¢å®Œæˆ: {search_time:.3f}ç§’ï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            
            return results
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def print_results(self, results: List[Dict[str, Any]], show_content: bool = True):
        """
        æ ¼å¼åŒ–æ‰“å°æŸ¥è¯¢ç»“æœ
        
        Args:
            results: æŸ¥è¯¢ç»“æœ
            show_content: æ˜¯å¦æ˜¾ç¤ºå®Œæ•´å†…å®¹
        """
        if not results:
            print("ğŸš« æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
            return
        
        print(f"\nğŸ“‹ æŸ¥è¯¢ç»“æœ (å…± {len(results)} æ¡):")
        print("=" * 70)
        
        for result in results:
            # è·å–æ£€ç´¢æ¥æºå’Œå¯¹åº”çš„å›¾æ ‡
            source = result.get('retrieval_source', 'unknown')
            source_icons = {
                'vector': 'ğŸ¯ å‘é‡æ£€ç´¢',
                'bm25': 'ğŸ”¤ å…³é”®è¯æ£€ç´¢', 
                'hybrid': 'ğŸ”¥ æ··åˆæ£€ç´¢',
                'unknown': 'â“ æœªçŸ¥æ¥æº'
            }
            source_display = source_icons.get(source, f'â“ {source}')
            
            print(f"ğŸ·ï¸ æ’å: {result['rank']}")
            print(f"ğŸ“„ æ–‡ä»¶: {result['filename']}")
            print(f"ğŸ“ è·¯å¾„: {result['file_path']}")
            print(f"â­ ç›¸ä¼¼åº¦: {result['score']:.4f}")
            print(f"ğŸ” æ£€ç´¢æ¥æº: {source_display}")
            print(f"ğŸ“Š é•¿åº¦: {result['content_length']} å­—ç¬¦")
            
            if show_content:
                content = result['content']
                if len(content) > 200:
                    content = content[:200] + "..."
                print(f"ğŸ“ å†…å®¹: {content}")
            
            print("-" * 70)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        doc_count = len(self.documents_metadata)
        total_chars = sum(doc['content_length'] for doc in self.documents_metadata)
        
        # ä¼°ç®—æ–‡æœ¬å—æ•°é‡ï¼ˆåŸºäºå¹³å‡å—å¤§å°300å­—ç¬¦ï¼‰
        estimated_chunks = total_chars // 300 if total_chars > 0 else 0
        
        return {
            'documents_loaded': doc_count,
            'total_characters': total_chars,
            'estimated_chunks': estimated_chunks,
            'knowledge_base_path': self.knowledge_base_path,
            'model_name': self.model_name,
            'chunk_size': Settings.chunk_size,
            'chunk_overlap': Settings.chunk_overlap
        }
    
    def performance_test(self, test_queries: List[str] = None) -> Dict[str, float]:
        """
        æ€§èƒ½æµ‹è¯•
        
        Args:
            test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            æ€§èƒ½ç»Ÿè®¡ç»“æœ
        """
        if not test_queries:
            test_queries = [
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                "æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
                "æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œ",
                "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯"
            ]
        
        print("âš¡ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        
        times = []
        for i, query in enumerate(test_queries, 1):
            print(f"ğŸ§ª æµ‹è¯•æŸ¥è¯¢ {i}: {query}")
            start_time = time.time()
            results = self.query(query, top_k=3)
            query_time = time.time() - start_time
            times.append(query_time)
            print(f"   â±ï¸ ç”¨æ—¶: {query_time:.3f}ç§’ï¼Œç»“æœæ•°: {len(results)}")
        
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        print(f"\nğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"   - å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_time:.3f}ç§’")
        print(f"   - æœ€å¿«æŸ¥è¯¢: {min_time:.3f}ç§’") 
        print(f"   - æœ€æ…¢æŸ¥è¯¢: {max_time:.3f}ç§’")
        
        return {
            'average_time': avg_time,
            'min_time': min_time,
            'max_time': max_time,
            'total_queries': len(test_queries)
        }
    
    def _scan_directory(self) -> set:
        """
        æ‰«æçŸ¥è¯†åº“ç›®å½•ï¼Œè¿”å›æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶è·¯å¾„é›†åˆ
        
        Returns:
            æ–‡ä»¶è·¯å¾„é›†åˆ
        """
        files = set()
        try:
            for root, _, filenames in os.walk(self.knowledge_base_path):
                for filename in filenames:
                    file_path = os.path.join(root, filename)
                    file_ext = Path(file_path).suffix.lower()
                    if file_ext in self._supported_extensions:
                        # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿å”¯ä¸€æ€§
                        files.add(os.path.abspath(file_path))
        except Exception as e:
            print(f"âš ï¸ æ‰«æç›®å½•å¤±è´¥: {e}")
        return files
    
    def _process_new_files(self, new_files: set) -> None:
        """
        å¤„ç†æ–°å‘ç°çš„æ–‡ä»¶
        
        Args:
            new_files: æ–°æ–‡ä»¶è·¯å¾„é›†åˆ
        """
        if not new_files:
            return
            
        print(f"\nğŸ” å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶:")
        for file_path in new_files:
            filename = os.path.basename(file_path)
            print(f"   ğŸ“„ {filename}")
        
        try:
            # é‡æ–°åŠ è½½æ–‡æ¡£ï¼ˆä¼šè‡ªåŠ¨æ£€æµ‹å¹¶åªå¤„ç†æ–°æ–‡ä»¶ï¼‰
            print("ğŸ”„ å¼€å§‹å¤„ç†æ–°æ–‡ä»¶...")
            old_doc_count = len(self.documents_metadata)  # è®°å½•å¤„ç†å‰çš„æ–‡æ¡£æ•°
            new_count = self.load_documents()
            
            # æ­£ç¡®çš„é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦æˆåŠŸæ·»åŠ äº†æ‰€æœ‰æ–°æ–‡ä»¶
            expected_count = old_doc_count + len(new_files)
            if new_count >= expected_count:
                print(f"âœ… æ–°æ–‡ä»¶å¤„ç†å®Œæˆï¼ŒçŸ¥è¯†åº“å·²æ›´æ–° ({old_doc_count} â†’ {new_count})")
            else:
                print(f"âš ï¸ éƒ¨åˆ†æ–‡ä»¶å¯èƒ½å¤„ç†å¤±è´¥ (é¢„æœŸ: {expected_count}, å®é™…: {new_count})")
                
            # æ›´æ–°å·²çŸ¥æ–‡ä»¶é›†åˆ
            self._known_files.update(new_files)
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–°æ–‡ä»¶å¤±è´¥: {e}")
    
    def _monitor_directory(self) -> None:
        """
        ç›‘æ§ç›®å½•å˜åŒ–çš„åå°çº¿ç¨‹å‡½æ•°
        """
        print(f"ğŸ“¡ å¼€å§‹ç›‘æ§çŸ¥è¯†åº“ç›®å½•: {self.knowledge_base_path}")
        print("â° ç›‘æ§é—´éš”: 5ç§’")
        
        # åˆå§‹æ‰«æ
        self._known_files = self._scan_directory()
        print(f"ğŸ“Š åˆå§‹æ–‡ä»¶æ•°é‡: {len(self._known_files)}")
        
        while self._monitoring:
            try:
                time.sleep(5)  # 5ç§’æ£€æµ‹é—´éš”
                
                if not self._monitoring:
                    break
                
                # æ‰«æå½“å‰æ–‡ä»¶
                current_files = self._scan_directory()
                
                # æ‰¾å‡ºæ–°æ–‡ä»¶
                new_files = current_files - self._known_files
                
                if new_files:
                    self._process_new_files(new_files)
                
                # æ£€æµ‹åˆ é™¤çš„æ–‡ä»¶ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
                deleted_files = self._known_files - current_files
                if deleted_files:
                    print(f"ğŸ—‘ï¸ æ£€æµ‹åˆ° {len(deleted_files)} ä¸ªæ–‡ä»¶è¢«åˆ é™¤")
                    # æ³¨æ„ï¼šè¿™é‡Œä¸å¤„ç†åˆ é™¤ï¼Œå› ä¸ºå‘é‡æ•°æ®åº“ä¸­çš„æ•°æ®ä¿æŒä¸å˜
                    # å¦‚éœ€å¤„ç†åˆ é™¤ï¼Œå¯ä»¥æ·»åŠ ç›¸åº”é€»è¾‘
                    self._known_files = current_files
                
            except Exception as e:
                print(f"âš ï¸ ç›‘æ§è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                time.sleep(5)  # å‡ºé”™åç­‰å¾…5ç§’ç»§ç»­
    
    def start_monitoring(self) -> None:
        """
        å¯åŠ¨æ–‡ä»¶ç›‘æ§
        """
        if self._monitoring:
            print("âš ï¸ æ–‡ä»¶ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return
        
        print("ğŸš€ å¯åŠ¨æ–‡ä»¶ç›‘æ§...")
        self._monitoring = True
        self._monitor_thread = threading.Thread(
            target=self._monitor_directory,
            daemon=True  # è®¾ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼Œä¸»ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨ç»“æŸ
        )
        self._monitor_thread.start()
        print("âœ… æ–‡ä»¶ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self) -> None:
        """
        åœæ­¢æ–‡ä»¶ç›‘æ§
        """
        if not self._monitoring:
            print("âš ï¸ æ–‡ä»¶ç›‘æ§æœªè¿è¡Œ")
            return
        
        print("ğŸ›‘ åœæ­¢æ–‡ä»¶ç›‘æ§...")
        self._monitoring = False
        
        if self._monitor_thread and self._monitor_thread.is_alive():
            self._monitor_thread.join(timeout=10)  # ç­‰å¾…æœ€å¤š10ç§’
        
        print("âœ… æ–‡ä»¶ç›‘æ§å·²åœæ­¢")
    
    def is_monitoring(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ­£åœ¨ç›‘æ§
        
        Returns:
            True if monitoring is active, False otherwise
        """
        return self._monitoring


def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    print("ğŸ¯ RAG Demo æ¼”ç¤ºç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag = RAGDemo(knowledge_base_path="data_test")
    
    # åŠ è½½æ–‡æ¡£
    doc_count = rag.load_documents()
    if doc_count == 0:
        print("âŒ æ²¡æœ‰åŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œè¯·åœ¨data_testç›®å½•ä¸­æ”¾å…¥æ–‡æ¡£æ–‡ä»¶")
        return
    
    # æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡
    stats = rag.get_stats()
    print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"   - {key}: {value}")
    
    # äº¤äº’å¼æŸ¥è¯¢
    print(f"\nğŸ” äº¤äº’å¼æŸ¥è¯¢ (è¾“å…¥ 'quit' é€€å‡º, 'test' è¿è¡Œæ€§èƒ½æµ‹è¯•):")
    
    while True:
        try:
            query = input("\nğŸ’¬ è¯·è¾“å…¥æŸ¥è¯¢é—®é¢˜: ").strip()
            
            if query.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            if query.lower() == 'test':
                rag.performance_test()
                continue
                
            if not query:
                continue
                
            # æ‰§è¡ŒæŸ¥è¯¢
            results = rag.query(query, top_k=3)
            rag.print_results(results, show_content=True)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å‡ºé”™: {e}")


if __name__ == "__main__":
    main()